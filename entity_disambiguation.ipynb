{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Disambiguation with an all-Born pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally tackle our target problem, i.e., entity disambiguation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from bornrule import BornClassifier\n",
    "\n",
    "from wiki_tools import get_data_from_snippets\n",
    "\n",
    "from models import MultilayerBornModel, FineTunedBornClassifier\n",
    "from utils import construct_traintest_dataframe, apply_categorical_mapping, get_mlp_data, get_latest_model, get_latest_vectoriser, get_latest_encmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data_from_snippets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first choose a target entity to disambiguate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_entity = random.choice(list(data_dict.keys()))\n",
    "target_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we collect a set of entities for training the disambiguating Multilayer Born model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# train_size is kept small to avoid excessive resourse utilisation\n",
    "traintest_df = construct_traintest_dataframe(nlp, target_entity, data_dict, train_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to make the dataset ameanable for the Born models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = traintest_df[traintest_df['entity'] != target_entity].reset_index(drop=True)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = traintest_df[traintest_df['entity'] == target_entity].reset_index(drop=True)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = get_latest_encmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_df = apply_categorical_mapping(train_df, mappings)\n",
    "enc_train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_test_df = apply_categorical_mapping(test_df, mappings)\n",
    "enc_test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we extract two targets: the targets for NER (for the Born Classifier), and the ones for NED (for the Multilayer Born model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ner_train = enc_train_df['ner_tag'].to_list()\n",
    "y_ner_test = enc_test_df['ner_tag'].to_list()\n",
    "enc_train_df.drop('ner_tag', axis=1)\n",
    "enc_test_df.drop('ner_tag', axis=1)\n",
    "\n",
    "y_train = enc_train_df['disambig_label'].to_list()\n",
    "y_test = enc_test_df['disambig_label'].to_list()\n",
    "enc_train_df.drop('disambig_label', axis=1)\n",
    "enc_test_df.drop('disambig_label', axis=1); # to suppress the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vec = get_latest_vectoriser()\n",
    "\n",
    "# we need the output to be as expected, i.e., we should only vectorise the columns we pre-trained the born classifier on\n",
    "cols_to_vectorise = ['sentence_id', 'token', 'pos', 'dep']\n",
    "\n",
    "X_train = dict_vec.transform(enc_train_df[cols_to_vectorise].to_dict('records'))\n",
    "X_test = dict_vec.transform(enc_test_df[cols_to_vectorise].to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with the Born Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Digression) Does fine-tuning help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that ensembling doesn't help much, so it's only natural to ask whether fine-tuning will.\n",
    "The cells below investigate this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model trained from scratch on the new data\n",
    "born_new = BornClassifier()\n",
    "born_new.fit(X_train, y_ner_train)\n",
    "\n",
    "# the model trained on the massive wikipedia data\n",
    "born_trained = get_latest_model('clf')\n",
    "\n",
    "# the fine-tuned model (the learning rate is set to an arbitrary value)\n",
    "n_classes = len(mappings['ner_tag'])\n",
    "born_fine = FineTunedBornClassifier(born_trained, n_classes=n_classes, learning_rate=0.8)\n",
    "born_fine.fit(X_train, y_ner_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = born_new.predict(X_test)\n",
    "print(classification_report(y_true=y_ner_test, y_pred=y_pred_new, labels=np.unique(y_train), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_trained = born_trained.predict(X_test)\n",
    "print(classification_report(y_true=y_ner_test, y_pred=y_pred_trained, labels=np.unique(y_train), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report above clearly motives the use of a pre-trained model: because our current dataset is quite small, we can have NER tags with very little support and, on these NER tags, the pre-trained model does much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fine = born_fine.predict(X_test)\n",
    "print(classification_report(y_true=y_ner_test, y_pred=y_pred_fine, labels=np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, from what we sated above, combining a small Born classifier with our pre-trained one should allow us to get the best of both worlds: good performance on the current data, because of the ex-novo Born classifier, and better generalisation due to the pre-trained model.\n",
    "\n",
    "As the classification report above shows! (Note that the advantage might not be so clear all the time, because each time the notebook is re-run there is randomness. To better see the difference, I suggest increasing `train_size` in the `construct_traintest_dataframe` above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the Born Classifier for NED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen that fine-tuning does, in fact, help, let's create the fine-tuned Born classifier and move on to NED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "born_pretrained = get_latest_model('clf')\n",
    "\n",
    "n_classes = len(mappings['ner_tag'])\n",
    "born_finetuned = FineTunedBornClassifier(born_pretrained, n_classes=n_classes, learning_rate=0.4)\n",
    "born_finetuned.fit(X_train, y_ner_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NED with Multilayer Born"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we keep train_size small mostly so that this and the following cells don't consume excessive resources\n",
    "# (even with train_size=2, this cell takes ~9m and training the model shows peaks of RAM usage touching ~30GB!)\n",
    "X_mlp_train, y_mlp_train = get_mlp_data(X_train, y_train, born_finetuned)\n",
    "X_mlp_test, y_mlp_test = get_mlp_data(X_test, y_test, born_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_mlp_train.shape[1] # ~600K (!)\n",
    "out_size = n_classes # 19\n",
    "\n",
    "# we reduce dimension aggressively because the input vectors are sparse \n",
    "# (TruncatedSVD results in an explained variance of ~0.99 with only 1000 features)\n",
    "layer_sizes = [input_size, 512, 64, out_size]\n",
    "model = MultilayerBornModel(layer_sizes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "dataset = TensorDataset(X_mlp_train, y_mlp_train.long())\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally train the Multilayer Born model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "\n",
    "batch_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_pbar = tqdm(dataloader, total=len(dataloader), desc=f\"Epoch {epoch+1}, Batch\", position=1, leave=False)\n",
    "    for batch_X, batch_y in batch_pbar:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        batch_pbar.set_postfix({'batch_loss': f'{loss.item():.4f}'})\n",
    "    display(batch_pbar)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] => Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could definitely benefit from more training...\n",
    "\n",
    "Still, that is not the main point. More importantly, we need to save our progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "\n",
    "mlp_path = os.path.join(\"runs\", f\"mlp_born_{timestamp}.pt\")\n",
    "torch.save(model.state_dict(), mlp_path)\n",
    "print(f\"[+] Multilayer Born saved to {mlp_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_mlp_train.shape[1] + 1\n",
    "out_size = n_classes\n",
    "layer_sizes = [input_size, 512, 64, out_size]\n",
    "model = MultilayerBornModel(layer_sizes)\n",
    "model.load_state_dict(get_latest_model(\"mlp\"))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred_proba = model(X_mlp_test)\n",
    "    \n",
    "    y_pred_labels = torch.argmax(y_pred_proba, dim=1)\n",
    "    \n",
    "    # convert tensors to numpy arrays for sklearn metrics\n",
    "    y_pred_proba_np = y_pred_proba.cpu().numpy()\n",
    "    y_pred_labels_np = y_pred_labels.cpu().numpy()\n",
    "    y_test_np = y_test.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_np, y_pred_labels_np)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    logloss = log_loss(y_test_np, y_pred_proba_np)\n",
    "    print(f\"Log Loss: {logloss:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test_np, y_pred_labels_np))\n",
    "\n",
    "# def predict_proba(model, X):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         probabilities = model(X)\n",
    "#     return probabilities\n",
    "\n",
    "# def predict(model, X):\n",
    "#     probabilities = predict_proba(model, X)\n",
    "#     return torch.argmax(probabilities, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bored",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
